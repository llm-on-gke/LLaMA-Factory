# Distributed training of a traditional CNN model to do image classification 
# using the MNIST dataset and PyTorch.
apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: pytorch
spec:
  replicatedJobs:
  - name: workers
    template:
      spec:
        parallelism: 2
        completions: 2
        backoffLimit: 0
        template:
          metadata:
             annotations:
               gke-gcsfuse/volumes: "true"
          spec:
            schedulingGates:
            - name: "gke.io/topology-aware-auto-scheduling-pytorch"
            hostNetwork: true
            dnsPolicy: ClusterFirstWithHostNet
            nodeSelector:
               cloud.google.com/gke-accelerator: nvidia-h100-mega-80gb
            serviceAccountName: storage-access
            volumes:
            - name: libraries
              hostPath:
                path: /home/kubernetes/bin/nvidia/lib64
            - name: sys
              hostPath:
                path: /sys
            - name: proc-sys
              hostPath:
                path: /proc/sys
            - name: aperture-devices
              hostPath:
                path: /dev/aperture_devices
            - name: data
              emptyDir: {}
            - name: hf-cache
              emptyDir: {}
            - name: model-cache
              emptyDir: {}
            - name: output
              emptyDir: {}
            - name: gcs-fuse-csi-ephemeral
              csi:
                driver: gcsfuse.csi.storage.gke.io
                readOnly: false
                volumeAttributes:
                   bucketName: rick-llama-factory
                   mountOptions: "implicit-dirs"
                   gcsfuseLoggingSeverity: warning
                   fileCacheCapacity: "200Gi"

            - name: dshm
              emptyDir:
                medium: Memory

            containers:
            - name: tcpxo-daemon
              image: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpxo/tcpgpudmarxd-dev:v1.0.11
              imagePullPolicy: Always
              command: ["/bin/sh", "-c"]
              args:
                - |
                  set -ex
                  chmod 755 /fts/entrypoint_rxdm_container.sh
                  /fts/entrypoint_rxdm_container.sh --num_hops=2 --num_nics=8 --uid= --alsologtostderr
              securityContext:
                 capabilities:
                   add:
                    - NET_ADMIN
                    - NET_BIND_SERVICE
              volumeMounts:
                - name: libraries
                  mountPath: /usr/local/nvidia
                - name: sys
                  mountPath: /hostsysfs
                - name: proc-sys
                  mountPath: /hostprocsysfs
              env:
                - name: LD_LIBRARY_PATH
                  value: /usr/local/nvidia/lib64

            - name: pytorch
              image: us-east1-docker.pkg.dev/northam-ce-mlai-tpu/gke-llm/llama-factory:latest
              imagePullPolicy: Always
              # image: gcr.io/k8s-staging-jobset/pytorch-mnist:latest
              ports:
              - containerPort: 3389
              env:
              - name: HF_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: huggingface
                    key: HF_TOKEN
              - name: HUGGING_FACE_HUB_TOKEN
                valueFrom:
                  secretKeyRef:
                     name: huggingface
                     key: HF_TOKEN
              - name: MASTER_ADDR
                value: "pytorch-workers-0-0.pytorch"
              - name: MASTER_PORT
                value: "3389"
              - name: NODE_COUNT
                value: "2"
              - name: NODE_RANK
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
              # Force python to not buffer output and write directly to stdout, so we can view training logs via `kubectl logs`.
              - name: PYTHONUNBUFFERED
                value: "0"                   
              - name: PYTHON_SCRIPT
                value: "experiments/diffusion/diffusion.py --config=<CONFIG_FILE> --trainer.num_nodes=<NODE_COUNT> --trainer.logger.name=<JOB_NAME> --trainer.devices=auto <CONFIG_OPTIONS> fit"
              - name: NCCL_FASTRAK_LLCM_DEVICE_DIRECTORY
                value: /dev/aperture_devices
              - name: LD_LIBRARY_PATH
                value: /usr/local/nvidia/lib64  
              # - name: TRITON_CACHE_DIR
              #   value: irreverent-datasets/torch-compiler-cache
              securityContext:
                privileged: true    
              command: 
              - bash
              - -xc
              - |
                NCCL_LIB_DIR="/usr/local/nvidia/lib64"
                source ${NCCL_LIB_DIR}/nccl-env-profile.sh
                #cp -r -f /data/llama3_lora_sft.yaml examples/train_lora/llama3_lora_sft.yaml
                FORCE_TORCHRUN=1 NNODES=$NNODES RANK=$NODE_RANK MASTER_ADDR=$MASTER_ADDR MASTER_PORT=$MASTER_PORT llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
              resources:
                requests:
                  cpu: "8"
                  memory: "25Gi"
                  ephemeral-storage: "25Gi"
                  nvidia.com/gpu: 8
                limits:
                  # cpu: "16"
                  # memory: "30Gi"
                  # ephemeral-storage: "30Gi"
                  nvidia.com/gpu: 8
              volumeMounts:
               - mountPath: /dev/shm
                 name: dshm
               - mountPath: /gcs-dir
                 name: gcs-fuse-csi-ephemeral
               - name: data
                 mountPath: /data
               - name: hf-cache
                 mountPath: /root/.cache/huggingface
               - name: model-cache
                 mountPath: /root/.cache/modelscope
               - name: output
                 mountPath: /app/output
               