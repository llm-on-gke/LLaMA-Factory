# Copyright 2024 Google Inc. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: nccl-test
spec:
  ttlSecondsAfterFinished: 1200
  suspend: False
  network:
    enableDNSHostnames: true
  replicatedJobs:
  - name: nccl
    template:
      spec:
        parallelism: 2
        completions: 2
        template:
          spec:
            # Limit benchmark run duration
            activeDeadlineSeconds: 3600
            restartPolicy: Never
            nodeSelector:
              #cloud.google.com/gke-nodepool: dws-a3-mega #for dws
               cloud.google.com/gke-accelerator: nvidia-h100-mega-80gb 
            tolerations:
            - key: "nvidia.com/gpu"
              operator: "Exists"
              effect: "NoSchedule"
            hostNetwork: true
            dnsPolicy: ClusterFirstWithHostNet
            #setHostnameAsFQDN: true
            volumes:
            - name: nvidia
              hostPath:
                path: /home/kubernetes/bin/nvidia
            - name: shared-memory
              emptyDir:
                medium: "Memory"
                sizeLimit: 250Gi
            initContainers:
            - name: gpu-healthcheck
              image: "nvidia/cuda:12.8.1-base-ubuntu20.04" #alpine:latest
              command: ["/bin/sh", "-c"]
              args:
              - |
                apk add --no-cache bash  # Install bash
                /bin/bash -c "set -ex
                NUM_GPUS=$(/usr/local/nvidia/bin/nvidia-smi --query-gpu=driver_version --format=csv,noheader,nounits | wc -l)
                if [ \${NUM_GPUS} -lt 8 ]; then
                  echo \"Error: Only \${NUM_GPUS} GPUs and expected 8\"
                  exit 1
                fi
                gpu_errors=(\$(/usr/local/nvidia/bin/nvidia-smi --query-gpu=ecc.errors.uncorrected.volatile.total --format=csv,noheader,nounits))
                for gpu_index in \${!gpu_errors[@]}; do
                    if [ \${gpu_errors[\$gpu_index]} == '[N/A]' ]; then
                        echo 'Error: ERR detected in GPU index '\$gpu_index
                        exit 1
                    elif [ \${gpu_errors[\$gpu_index]} -gt 0 ]; then
                        echo 'Error: Unrecoverable ECC errors detected in GPU index '\$gpu_index
                        exit 1
                    fi
                done
                echo \${NUM_GPUS} GPUs found with no ERR or Unrecoverable ECC errors"

              volumeMounts:
              - name: nvidia
                mountPath: /usr/local/nvidia
              securityContext:
                privileged: true
              env:
              - name: LD_LIBRARY_PATH
                value: /usr/local/nvidia/lib64

            containers:
            - name: tcpxo-daemon
              image: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpxo/tcpgpudmarxd-dev:v1.0.14
              imagePullPolicy: Always
              command: ["/bin/sh", "-c"]
              args:
                 - |
                   set -ex
                   chmod 755 /fts/entrypoint_rxdm_container.sh
                   /fts/entrypoint_rxdm_container.sh --num_hops=2 --num_nics=8 --uid= --alsologtostderr
              securityContext:
                 privileged: true
              volumeMounts:
                - name: nvidia
                  mountPath: /usr/local/nvidia
              env:
                - name: LD_LIBRARY_PATH
                  value: /usr/local/nvidia/lib64
            - name: nccl
              stdin: true
              tty: true
              image: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpxo/nccl-plugin-gpudirecttcpx-dev:v1.0.8-1
              securityContext:
                privileged: true
              env:
              - name: LD_LIBRARY_PATH
                value: /usr/local/nvidia/lib64
              - name: hostname_prefix
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['jobset.sigs.k8s.io/jobset-name']
              - name: hostname_suffix
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['jobset.sigs.k8s.io/replicatedjob-name']
                
              - name: OMPI_ALLOW_RUN_AS_ROOT
                value: "1"
              - name: OMPI_ALLOW_RUN_AS_ROOT_CONFIRM
                value: "1"
              command:
              - bash
              - -c
              - |
                set -x
                export N_NODES=2
                echo "Starting workload container on ${MY_NODE_NAME} for $N_NODES benchmark"

                # Load all the cuda libs
                /sbin/ldconfig

                # Install ping
                apt update -y
                apt install -y iputils-ping

                # Start sshd
                /scripts/container_entry.sh daemon &

                # Get helper variables to form all hostnames
                #export POSTFIX=$(hostname | cut -d . -f 2-)
                #export WORKERS_BASENAME=$(hostname | cut -d . -f 1 | rev | cut -d - -f 2- | rev )
                export NODE_RANK=$JOB_COMPLETION_INDEX


                # For every worker, wait till online and add to hostfile
                for i in `seq 0 $(($N_NODES-1))`; do
                  OTHER=${hostname_prefix}-0-${i}.${hostname_suffix}
                  until ssh -p 222 -o StrictHostKeyChecking=no $OTHER hostname; do
                    echo Waiting for ${OTHER}...
                    sleep 10
                  done
                  echo ${OTHER} port=222 slots=8 | tee -a /tmp/hostfile;
                done

                cat /tmp/hostfile

                # Launch from head node
                if [[ "${NODE_RANK}" -eq "0" ]]; then

                    # World Level = 0x0, Rail Aligned = 0x7
                    export NCCL_TESTS_SPLIT_MASK="0x0";

                    # Force use of libnccl-gib
                    #export NCCL_NET=gIB
                    export NCCL_LIB_DIR="/usr/local/nvidia/lib64"
                    # Set all the correct libnccl-gib environment variables
                    #source /usr/local/gib/scripts/set_nccl_env.sh
                    source ${NCCL_LIB_DIR}/nccl-env-profile-ll128.sh
                    # Get all relevant NCCL / env vars to pass to all workers
                    ENV_VARS=$(echo ${!NCCL*} ${!OMPI*} LD_LIBRARY_PATH PATH | sed 's/ / -x /g')

                    mpirun --hostfile /tmp/hostfile \
                      -x $ENV_VARS  \
                      -mca plm_rsh_no_tree_spawn 1 \
                      --mca orte_keep_fqdn_hostnames 1 \
                      --mca btl self,tcp \
                      --mca btl_tcp_if_include eth0 \
                      --bind-to none \
                      --mca plm_rsh_agent "ssh -q -o LogLevel=ERROR -o StrictHostKeyChecking=no -p 222" \
                      /third_party/nccl-tests/build/all_gather_perf -b 1K -e 8G -f 2 -g 1 -w 5 --iters 100 -c 1

                else
                    while ping -c 1 ${WORKERS_BASENAME}-0.${POSTFIX}; do
                    sleep 5
                done
                fi

                exit 0

              volumeMounts:
              - name: nvidia
                mountPath: /usr/local/nvidia
              - name: shared-memory
                mountPath: /dev/shm
              resources:
                limits:
                  nvidia.com/gpu: 8
                requests:
                  nvidia.com/gpu: 8