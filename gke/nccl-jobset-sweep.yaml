# Copyright 2024 Google Inc. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: nccl-test
spec:
  ttlSecondsAfterFinished: 1200
  suspend: False
  network:
    enableDNSHostnames: true
  replicatedJobs:
  - name: nccl
    template:
      spec:
        parallelism: 2
        completions: 2
        template:
          spec:
            # Limit benchmark run duration
            activeDeadlineSeconds: 3600
            restartPolicy: Never
            nodeSelector:
              #cloud.google.com/gke-nodepool: dws-a3-mega #for dws
               cloud.google.com/gke-accelerator: nvidia-h100-mega-80gb 
            tolerations:
            - key: "nvidia.com/gpu"
              operator: "Exists"
              effect: "NoSchedule"
            hostNetwork: true
            dnsPolicy: ClusterFirstWithHostNet
            #setHostnameAsFQDN: true
            volumes:
            - name: nvidia
              hostPath:
                path: /home/kubernetes/bin/nvidia
            - name: shared-memory
              emptyDir:
                medium: "Memory"
                sizeLimit: 250Gi
            #initContainers:
            #- name: gpu-healthcheck
            #  image: "nvidia/cuda:12.8.1-base-ubuntu20.04" #alpine:latest
            #  command: ["/bin/sh", "-c"]
            #  args:
            #  - |
            #    apk add --no-cache bash  # Install bash
            #    /bin/bash -c "set -ex
            #    NUM_GPUS=$(/usr/local/nvidia/bin/nvidia-smi --query-gpu=driver_version --format=csv,noheader,nounits | wc -l)
            #    if [ \${NUM_GPUS} -lt 8 ]; then
            #      echo \"Error: Only \${NUM_GPUS} GPUs and expected 8\"
            #      exit 1
            #    fi
            #    gpu_errors=(\$(/usr/local/nvidia/bin/nvidia-smi --query-gpu=ecc.errors.uncorrected.volatile.total --format=csv,noheader,nounits))
            #    for gpu_index in \${!gpu_errors[@]}; do
            #        if [ \${gpu_errors[\$gpu_index]} == '[N/A]' ]; then
            #            echo 'Error: ERR detected in GPU index '\$gpu_index
            #            exit 1
            #        elif [ \${gpu_errors[\$gpu_index]} -gt 0 ]; then
            #            echo 'Error: Unrecoverable ECC errors detected in GPU index '\$gpu_index
            #            exit 1
            #        fi
            #    done
            #    echo \${NUM_GPUS} GPUs found with no ERR or Unrecoverable ECC errors"

            #  volumeMounts:
            #  - name: nvidia
            #    mountPath: /usr/local/nvidia
            #  securityContext:
            #    privileged: true
            #  env:
            #  - name: LD_LIBRARY_PATH
            #    value: /usr/local/nvidia/lib64

            containers:
            - name: tcpxo-daemon
              image: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpxo/tcpgpudmarxd-dev:v1.0.14
              imagePullPolicy: Always
              command: ["/bin/sh", "-c"]
              args:
                 - |
                   set -ex
                   chmod 755 /fts/entrypoint_rxdm_container.sh
                   /fts/entrypoint_rxdm_container.sh --num_hops=2 --num_nics=8 --uid= --alsologtostderr
              securityContext:
                 privileged: true
              volumeMounts:
                - name: nvidia
                  mountPath: /usr/local/nvidia
              env:
                - name: LD_LIBRARY_PATH
                  value: /usr/local/nvidia/lib64
            - name: nccl
              stdin: true
              tty: true
              image: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpxo/nccl-plugin-gpudirecttcpx-dev:v1.0.8-1
              securityContext:
                privileged: true
              env:
              - name: LD_LIBRARY_PATH
                value: /usr/local/nvidia/lib64
              - name: N_NODES
                value: "2"
              - name: hostname_prefix
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['jobset.sigs.k8s.io/jobset-name']
              - name: hostname_suffix
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['jobset.sigs.k8s.io/replicatedjob-name']
                
              - name: OMPI_ALLOW_RUN_AS_ROOT
                value: "1"
              - name: OMPI_ALLOW_RUN_AS_ROOT_CONFIRM
                value: "1"
              command:
              - bash
              - -c
              - |
                set -x
        
                echo "Starting workload container on ${MY_NODE_NAME} for $N_NODES benchmark"

                # Load all the cuda libs
                /sbin/ldconfig

                # Install ping
                apt update -y
                apt install -y iputils-ping

                # Start sshd
                /scripts/container_entry.sh daemon &
                cp /scripts/demo_mpi_entry_with_config_profile.sh /tmp/demo_mpi_entry_with_config_profile.sh
                echo "source source ${NCCL_LIB_DIR}/nccl-env-profile-ll128.sh" >> /tmp/demo_mpi_entry_with_config_profile.sh
                chmod +x /tmp/scripts/demo_mpi_entry_with_config_profile.sh 
                # Get helper variables to form all hostnames
                #export POSTFIX=$(hostname | cut -d . -f 2-)
                #export WORKERS_BASENAME=$(hostname | cut -d . -f 1 | rev | cut -d - -f 2- | rev )
                export NODE_RANK=$JOB_COMPLETION_INDEX

                if [[ "${NODE_RANK}" -eq "0" ]]; then
                  # For every worker, wait till online and add to hostfile
                  for i in `seq 0 $(($N_NODES-1))`; do
                    firsthost=${hostname_prefix}-${hostname_suffix}-0-${i}.${hostname_prefix}
                    until ssh -p 222 -o StrictHostKeyChecking=no $firsthost hostname; do
                     echo Waiting for ${firsthost}...
                     sleep 10
                    done
                    echo ${firsthost} port=222 slots=8 | tee /tmp/hostfile;
                    next=$((i+1))
                    echo "Processing pair nodes: ${i}"
                    if [[ "${next}" -lt "${N_NODES}" ]]; then
                      secondhost=${hostname_prefix}-${hostname_suffix}-0-${next}.${hostname_prefix}
                      until ssh -p 222 -o StrictHostKeyChecking=no $secondhost hostname; do
                        echo Waiting for ${secondhost}...
                        sleep 10
                      done
                    else
                      secondhost=${hostname_prefix}-${hostname_suffix}-0-0.${hostname_prefix}
                    fi
                    echo ${secondhost} port=222 slots=8 | tee -a /tmp/hostfile;
                    cat /tmp/hostfile
                    #launch 2 node job
                    # World Level = 0x0, Rail Aligned = 0x7
                    export NCCL_TESTS_SPLIT_MASK="0x0";
                    
                    # Force use of libnccl-gib
                    #export NCCL_NET=gIB
                    export NCCL_LIB_DIR="/usr/local/nvidia/lib64"
                    source ${NCCL_LIB_DIR}/nccl-env-profile-ll128.sh
                    # Get all relevant NCCL / env vars to pass to all workers
                    export ENV_VARS=$(echo ${!NCCL*} ${!OMPI*} LD_LIBRARY_PATH PATH | sed 's/ / -x /g')
                    export BENCHMARK=all_gather_perf
                    DATA_MIN="${DATA_MIN:-8}"
                    DATA_MAX="${DATA_MAX:-8G}"
                    GPU_PER_NODE="${GPU_PER_NODE:-8}"
                    RUN_ITERS="${RUN_ITERS:-20}"
                    WARMUP_ITERS="${WARMUP_ITERS:-5}"
                    #sleep infinity;
                    mpirun --mca btl tcp,self --mca btl_tcp_if_include eth0 --allow-run-as-root \
                    --mca orte_keep_fqdn_hostnames 1 \
                    -np $(( GPU_PER_NODE * 2 )) \
                    --hostfile /tmp/hostfile \
                    -x NCCL_DEBUG_FILE="/tmp/${BENCHMARK}"-%h-%p.log \
                    -x NCCL_TOPO_DUMP_FILE="/tmp/${BENCHMARK}"_topo.txt \
                    -x NCCL_GRAPH_DUMP_FILE="/tmp/${BENCHMARK}"_graph.txt \
                    -x LD_LIBRARY_PATH -x PATH \
                    -x NCCL_DEBUG=INFO -x NCCL_DEBUG_SUBSYS=INIT,NET \
                    -x NCCL_TESTS_SPLIT_MASK="${NCCL_TESTS_SPLIT_MASK:-0x0}" \
                    -x NCCL_FASTRAK_LLCM_DEVICE_DIRECTORY="${NCCL_FASTRAK_LLCM_DEVICE_DIRECTORY}" \
                    -x NCCL_LIB_DIR="${NCCL_LIB_DIR}" \
                    --mca plm_rsh_agent "ssh -q -o LogLevel=ERROR -o StrictHostKeyChecking=no -p 222" \
                    taskset -c 32-63 /tmp/demo_mpi_entry_with_config_profile.sh "${BENCHMARK}" \
                    -b "${DATA_MIN}" -e "${DATA_MAX}" -f 2 -g 1 -w "${WARMUP_ITERS}" --iters "${RUN_ITERS}" 2>&1 | \
                    tee "/tmp/${BENCHMARK}_nh${N_NODES}_ng${GPU_PER_NODE}_i${RUN_ITERS}.txt"
                  done
                else
                    while ping -c 1 ${hostname_prefix}-${hostname_suffix}-0-0.${hostname_prefix}; do
                     sleep 5
                    done
                fi

                exit 0

              volumeMounts:
              - name: nvidia
                mountPath: /usr/local/nvidia
              - name: shared-memory
                mountPath: /dev/shm
              resources:
                limits:
                  nvidia.com/gpu: 8
                requests:
                  nvidia.com/gpu: 8